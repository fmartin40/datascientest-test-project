{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Etapes de scrap du site [the muse](https://www.themuse.com/).\\\n",
    "On utilise au maximum les json récupéré dans les appels api site pour avoir des données structurées\n",
    "\n",
    "### Il y a 3 étapes principales. \n",
    "\n",
    "1.  requete sur un type de job exemple : \"data engineer\".\n",
    "    - method: get\n",
    "    - url : https://www.themuse.com/api/search-renderer/jobs?\n",
    "    - params : ctsEnabled=false&query=Data+Engineer&preference=krcbqorfvz&limit=20&timeout=5000\n",
    "    - recupération du json qui présente les différentes offres. Données conservées\n",
    "        - job_title\n",
    "        - company.short_name\n",
    "        - short_title\n",
    "        - posted_at\n",
    "        - cursor (le dernier cursor est utile pour la pagination) --> start_after = dernier cursor\n",
    "        - has_more (utile pour la pagination)\n",
    "\n",
    "2.  récupération de chaque job dans le json reçu et requete pour obtenir le html de chaque job\n",
    "    - method: get\n",
    "    - url : https://www.themuse.com/jobs/\n",
    "    - params: [hit.company.short_name]/[hit.short_title]\n",
    "\n",
    "3.  dans le html, recupérér le json\n",
    "    - dans la balise <script id=\"__NEXT_DATA__\" type=\"application/json\"></script>\n",
    "\n",
    "### données conservées\n",
    "\n",
    "Pour l'instant on conservce les données suivantes :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from string import Template\n",
    "\n",
    "site_url: str = \"https://www.themuse.com\"\n",
    "search_url: str =\"/api/search-renderer/jobs\"\n",
    "job_url: str = Template(\"/jobs/$company/$job_title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import re\n",
    "\n",
    "async def fetch_list_jobs(query: str, limit: int = 20, next: str | None = None)-> List[Dict]:\n",
    "    \"\"\" Effectue une requête pour lister les jobs.\n",
    "        Dans le cas d'une pagination, on ajoute start_after= cursor du dernier job\n",
    "\n",
    "    Args:\n",
    "        query (str): requête dont les espaces sont remplacés par des +\n",
    "        limit (int, optional): Limite de réponse max. Defaults à 20.\n",
    "        next (str | None, optional): cursor à partir duquel est lancé la requet si pagination. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: liste des jobs\n",
    "    \"\"\"\n",
    "    query:str = re.sub(r'\\s+','+', query)\n",
    "    search_params: Dict = {\n",
    "                        \"ctsEnabled\":\"false\",\n",
    "                        \"query\":query,\n",
    "                        \"preference\":\"krcbqorfvz\",\n",
    "                        \"limit\":limit,\n",
    "                        \"timeout\":5000\n",
    "                    }\n",
    "    if next :\n",
    "        search_params.update({\"start_after\":next})\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(f\"{site_url}{search_url}\", params=search_params) as resp:\n",
    "                if resp.status == 200:\n",
    "                    return await resp.json()\n",
    "        except Exception as exc :\n",
    "            print(exc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction des données à conserver dans les jobs\n",
    "# on peut utiliser glom pour l'extraction des json\n",
    "from typing import Tuple\n",
    "from glom import glom\n",
    "\n",
    "async def extract_resumed_jobs(response_api: Dict)-> Tuple[bool, List[ResumedJob]]: # type: ignore\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        response_api (Dict): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[ResumedJob]: _description_\n",
    "    \"\"\"\n",
    "    job_specs = {\n",
    "        \"title\":\"hit.title\",\n",
    "        \"short_title\": \"hit.short_title\",\n",
    "        \"company_name\": \"hit.company.short_name\",\n",
    "        \"score\": \"score\",\n",
    "        \"cursor\": \"cursor\",\n",
    "        \"posted_at\": \"hit.posted_at\",\n",
    "    }\n",
    "\n",
    "    resumed_jobs: List[Dict] = [glom(data,  job_specs) for data in response_api.get('hits')]\n",
    "    \n",
    "    return (response_api.get('has_more'), sorted(resumed_jobs, key=lambda k:k.score, reverse=False ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResumedJob(title='Data Engineer', short_title='data-engineer-85af17', company_name='appfire', score=91.36105, cursor='91.36105,1726551876000,2f06a4ce-6992-4de1-8b9a-3359ad458e45', posted_at=1726551876),\n",
       " ResumedJob(title='Data Engineer', short_title='data-engineer', company_name='constellationbrands', score=91.961586, cursor='91.961586,1727109629000,02ce7182-bbe9-45b3-8e12-b485ed308c09', posted_at=1727109629),\n",
       " ResumedJob(title='Data Engineer', short_title='data-engineer-d8bdf7', company_name='leidos', score=92.13994, cursor='92.13994,1727280846000,1d5c7933-ebfe-4f5a-aa7c-c4ac76da24ca', posted_at=1727280846),\n",
       " ResumedJob(title='Data Engineer', short_title='data-engineer-e67071', company_name='atlassian', score=92.154686, cursor='92.154686,1727915038000,bd22fc4b-7c06-46ee-8c3a-dfe9914eb271', posted_at=1727915038),\n",
       " ResumedJob(title='Data Engineer', short_title='data-engineer-950ad7', company_name='arcadia', score=92.484505, cursor='92.484505,1718201808000,bd345d35-45d8-48bc-84e5-aeed22c43fde', posted_at=1718201808)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# appel de la methode une 1ere fois\n",
    "resp: List[Dict] = await fetch_list_jobs(\"Data engineer\", limit=5)\n",
    "extract_jobs: Tuple[bool, List[ResumedJob]] = await extract_resumed_jobs(resp)\n",
    "\n",
    "resumed_jobs: List[ResumedJob] = extract_jobs[1]\n",
    "resumed_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation des ENtités qui structurent les données et permettent une conservation en base\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ResumedJob:\n",
    "    title: str\n",
    "    short_title: str\n",
    "    company_name: str\n",
    "    score: int # pour le tri\n",
    "    cursor: str\n",
    "    posted_at: int # timestamp\n",
    "\n",
    "resumed_jobs = [ResumedJob(**job) for job in resumed_jobs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
