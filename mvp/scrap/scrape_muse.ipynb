{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Etapes de scrap du site [the muse](https://www.themuse.com/).\\\n",
    "On utilise au maximum les json récupéré dans les appels api site pour avoir des données structurées\n",
    "\n",
    "### Il y a 3 étapes principales. \n",
    "\n",
    "1.  requete sur un type de job exemple : \"data engineer\".\n",
    "    - method: get\n",
    "    - url : https://www.themuse.com/api/search-renderer/jobs?\n",
    "    - params : ctsEnabled=false&query=Data+Engineer&preference=krcbqorfvz&limit=20&timeout=5000\n",
    "    - recupération du json qui présente les différentes offres. Données conservées\n",
    "        - job_title\n",
    "        - company.short_name\n",
    "        - short_title\n",
    "        - posted_at\n",
    "        - cursor (le dernier cursor est utile pour la pagination) --> start_after = dernier cursor\n",
    "        - has_more (utile pour la pagination)\n",
    "\n",
    "2.  récupération de chaque job dans le json reçu et requete pour obtenir le html de chaque job\n",
    "    - method: get\n",
    "    - url : https://www.themuse.com/jobs/\n",
    "    - params: [hit.company.short_name]/[hit.short_title]\n",
    "\n",
    "3.  dans le html, recupérér le json\n",
    "    - dans la balise <script id=\"__NEXT_DATA__\" type=\"application/json\"></script>\n",
    "\n",
    "### données conservées\n",
    "\n",
    "Pour l'instant on conservce les données suivantes :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from string import Template\n",
    "from typing import List\n",
    "\n",
    "site_url: str = \"https://www.themuse.com\"\n",
    "search_url: str =\"/api/search-renderer/jobs\"\n",
    "job_url: Template = Template(\"/jobs/$company/$job_name\")\n",
    "proxies_url:str = \"https://api.buyproxies.org\"\n",
    "proxies_api_key: str = \"5c38699e3dd06dc81f32a2ce7e8bb091\"\n",
    "proxies: List[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appel des proxies\n",
    "\n",
    "from pprint import pprint\n",
    "from typing import Dict\n",
    "import aiohttp\n",
    "\n",
    "headers = {\n",
    "     \"Accept\": \"plain/text\"\n",
    "}\n",
    "params:Dict =dict(\n",
    "    a=\"showProxies\",\n",
    "    pid=184389,\n",
    "    key=proxies_api_key,\n",
    "    port=12345\n",
    ")\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    try:\n",
    "        async with session.get(url=proxies_url, params=params) as resp:\n",
    "            if resp.status == 200:\n",
    "                proxies_text = await resp.text()\n",
    "                proxies = proxies_text.split('\\n')\n",
    "                proxies = [f'https://{proxy.split(':')[2]}:{proxy.split(':')[3]}@{proxy.split(':')[0]}:{proxy.split(':')[1]}'\n",
    "                      for proxy in proxies\n",
    "                      if proxy ]\n",
    "    except Exception as exc :\n",
    "            print(exc)\n",
    "\n",
    "user_agents: List[str]= [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.5 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.53 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Windows; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Windows; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.53 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Windows; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.53 Safari/537.36\"\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import re\n",
    "import random\n",
    "\n",
    "async def request_jobs_api(query: str,  next_page: str | None = None)-> List[Dict]:\n",
    "    \"\"\" Effectue une requête pour lister les jobs.\n",
    "        Dans le cas d'une pagination, on ajoute start_after= cursor du dernier job\n",
    "\n",
    "    Args:\n",
    "        query (str): requête dont les espaces sont remplacés par des +\n",
    "        limit (int, optional): Limite de réponse max. Defaults à 20.\n",
    "        next (str | None, optional): cursor à partir duquel est lancé la requete si pagination. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: liste des jobs\n",
    "    \"\"\"\n",
    "    query:str = re.sub(r'\\s+','+', query)\n",
    "    search_params: Dict = {\n",
    "                        \"ctsEnabled\":\"false\",\n",
    "                        \"query\":query,\n",
    "                        \"preference\":\"krcbqorfvz\",\n",
    "                        \"limit\":20,\n",
    "                        \"timeout\":5000\n",
    "                    }\n",
    "    if next_page :\n",
    "        search_params.update({\"start_after\":next_page})\n",
    "\n",
    "    # proxy: str = random.choice(proxies)\n",
    "    user_agent: str = random.choice(user_agents)\n",
    "    headers = {\"User-Agent\": user_agent, \"Accept\": \"application/json\"}\n",
    "    url:str = f\"{site_url}{search_url}\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        \n",
    "        try:\n",
    "            async with session.get(url=url, params=search_params, headers=headers) as resp:\n",
    "                if resp.status == 200:\n",
    "                    return await resp.json()\n",
    "                    \n",
    "        except Exception as exc :\n",
    "            print(exc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction des données à conserver dans les jobs\n",
    "# on peut utiliser glom pour l'extraction des json\n",
    "from typing import Tuple\n",
    "from glom import glom\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# creation des Entités qui structurent les données et permettent une conservation en base\n",
    "@dataclass\n",
    "class JobUrl:\n",
    "    company: str\n",
    "    job_name:str\n",
    "    url: str  = field(init=False)\n",
    "    processed: bool = False\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.url = job_url.safe_substitute(company=self.company, job_name=self.job_name)\n",
    "        \n",
    "\n",
    "async def scrap_jobs(response_api: Dict)-> Dict: # type: ignore\n",
    "    \"\"\"Extrait les jobs a partir la reponse de l'api qui liste les jobs\n",
    "\n",
    "    Args:\n",
    "        response_api (Dict): dictionnaire renvoyé par l'api\n",
    "\n",
    "    Returns:\n",
    "        Dict: dictionnaire comprenant les différentes information résultant du scrap\n",
    "    \"\"\"\n",
    "    job_specs = {\n",
    "        \"company\": \"hit.company.short_name\",\n",
    "        \"job_name\": \"hit.short_title\",\n",
    "        \"score\": \"score\",\n",
    "        \"cursor\": \"cursor\"\n",
    "    }\n",
    "    job_list: List[Dict] = [glom(job,  job_specs) for job in response_api.get('hits')]\n",
    "    job_list = sorted(job_list, key=lambda k:k['score'], reverse=True )\n",
    "    job_urls: List[JobUrl] = [JobUrl(company=job.get('company'), job_name=job.get('job_name')) for job in job_list]\n",
    "    \n",
    "    next_page: str = job_list[-1].get('cursor')\n",
    "    has_more: bool = response_api.get('has_more', False)\n",
    "\n",
    "    return dict(has_more=has_more, next_page=next_page, job_urls=job_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "84.74229,1727281535000,04b3be51-2b3d-45a6-90ad-ed17a47db205\n",
      "True\n",
      "65.37807,1690931340000,2262703c-b2b6-4ade-9bc3-7022f659b9e0\n",
      "True\n",
      "64.38387,1727125877000,d53ec78a-3681-4671-8536-db0a4ffc43af\n",
      "60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[JobUrl(company='arcadia', job_name='data-engineer-950ad7', url='/jobs/arcadia/data-engineer-950ad7', processed=False),\n",
       " JobUrl(company='leidos', job_name='data-engineer-d8bdf7', url='/jobs/leidos/data-engineer-d8bdf7', processed=False),\n",
       " JobUrl(company='appfire', job_name='data-engineer', url='/jobs/appfire/data-engineer', processed=False),\n",
       " JobUrl(company='constellationbrands', job_name='data-engineer', url='/jobs/constellationbrands/data-engineer', processed=False),\n",
       " JobUrl(company='coinbase', job_name='data-engineer-0b1092', url='/jobs/coinbase/data-engineer-0b1092', processed=False),\n",
       " JobUrl(company='appfire', job_name='data-engineer-85af17', url='/jobs/appfire/data-engineer-85af17', processed=False),\n",
       " JobUrl(company='kyndryl', job_name='data-engineer-f6b05a', url='/jobs/kyndryl/data-engineer-f6b05a', processed=False),\n",
       " JobUrl(company='atlassian', job_name='data-engineer-ii', url='/jobs/atlassian/data-engineer-ii', processed=False),\n",
       " JobUrl(company='arcadia', job_name='senior-data-engineer', url='/jobs/arcadia/senior-data-engineer', processed=False),\n",
       " JobUrl(company='nationwideinsurance', job_name='data-engineer-specialist', url='/jobs/nationwideinsurance/data-engineer-specialist', processed=False),\n",
       " JobUrl(company='atlassian', job_name='principle-data-engineer-9b277a', url='/jobs/atlassian/principle-data-engineer-9b277a', processed=False),\n",
       " JobUrl(company='kyndryl', job_name='azure-data-engineer-845873', url='/jobs/kyndryl/azure-data-engineer-845873', processed=False),\n",
       " JobUrl(company='zendesk', job_name='senior-data-engineer-8a3f16', url='/jobs/zendesk/senior-data-engineer-8a3f16', processed=False),\n",
       " JobUrl(company='nationwideinsurance', job_name='consultant-analytics-data-engineer', url='/jobs/nationwideinsurance/consultant-analytics-data-engineer', processed=False),\n",
       " JobUrl(company='himshers', job_name='sr-data-engineer-752c15', url='/jobs/himshers/sr-data-engineer-752c15', processed=False),\n",
       " JobUrl(company='worldwidetechnology', job_name='data-engineer-costa-rica-188171', url='/jobs/worldwidetechnology/data-engineer-costa-rica-188171', processed=False),\n",
       " JobUrl(company='atlassian', job_name='senior-data-engineer-725172', url='/jobs/atlassian/senior-data-engineer-725172', processed=False),\n",
       " JobUrl(company='gamechanger', job_name='senior-data-engineer-7a45de', url='/jobs/gamechanger/senior-data-engineer-7a45de', processed=False),\n",
       " JobUrl(company='coinbase', job_name='senior-data-engineer-7493cf', url='/jobs/coinbase/senior-data-engineer-7493cf', processed=False),\n",
       " JobUrl(company='enterprisemobility', job_name='senior-data-engineer-data-modeling', url='/jobs/enterprisemobility/senior-data-engineer-data-modeling', processed=False),\n",
       " JobUrl(company='generalmotors', job_name='staff-data-engineer-activation-science', url='/jobs/generalmotors/staff-data-engineer-activation-science', processed=False),\n",
       " JobUrl(company='generalmotors', job_name='senior-data-engineer-measurement-science-2c5e37', url='/jobs/generalmotors/senior-data-engineer-measurement-science-2c5e37', processed=False),\n",
       " JobUrl(company='crowdstrike', job_name='data-engineer-iii-remote-ind', url='/jobs/crowdstrike/data-engineer-iii-remote-ind', processed=False),\n",
       " JobUrl(company='crowdstrike', job_name='marketing-data-engineer-remote', url='/jobs/crowdstrike/marketing-data-engineer-remote', processed=False),\n",
       " JobUrl(company='meta', job_name='data-engineer-product-analytics-3dcc13', url='/jobs/meta/data-engineer-product-analytics-3dcc13', processed=False),\n",
       " JobUrl(company='kyndryl', job_name='data-engineer-python-aws', url='/jobs/kyndryl/data-engineer-python-aws', processed=False),\n",
       " JobUrl(company='goodrx', job_name='sr-data-engineer-platform', url='/jobs/goodrx/sr-data-engineer-platform', processed=False),\n",
       " JobUrl(company='kyndryl', job_name='senior-lead-data-engineer-workforce-analytics-c1c2d8', url='/jobs/kyndryl/senior-lead-data-engineer-workforce-analytics-c1c2d8', processed=False),\n",
       " JobUrl(company='capitalone', job_name='data-engineer-capital-one-software-remote-c9f213', url='/jobs/capitalone/data-engineer-capital-one-software-remote-c9f213', processed=False),\n",
       " JobUrl(company='hatch', job_name='data-platform-engineer', url='/jobs/hatch/data-platform-engineer', processed=False),\n",
       " JobUrl(company='himshers', job_name='principal-engineer-data', url='/jobs/himshers/principal-engineer-data', processed=False),\n",
       " JobUrl(company='autodesk', job_name='research-engineer-datasets-3d-data', url='/jobs/autodesk/research-engineer-datasets-3d-data', processed=False),\n",
       " JobUrl(company='coinbase', job_name='senior-software-engineer-data-platform-37579a', url='/jobs/coinbase/senior-software-engineer-data-platform-37579a', processed=False),\n",
       " JobUrl(company='autodesk', job_name='data-architect-93ab18', url='/jobs/autodesk/data-architect-93ab18', processed=False),\n",
       " JobUrl(company='nvidia', job_name='senior-software-engineer-data-ingestion-autonomous-vehicles-43c52a', url='/jobs/nvidia/senior-software-engineer-data-ingestion-autonomous-vehicles-43c52a', processed=False),\n",
       " JobUrl(company='nvidia', job_name='principal-firmware-engineer-data-center-server-management', url='/jobs/nvidia/principal-firmware-engineer-data-center-server-management', processed=False),\n",
       " JobUrl(company='autodesk', job_name='principal-platform-engineer-cloud-infrastructure-big-data', url='/jobs/autodesk/principal-platform-engineer-cloud-infrastructure-big-data', processed=False),\n",
       " JobUrl(company='zendesk', job_name='senior-data-scientist-485e8f', url='/jobs/zendesk/senior-data-scientist-485e8f', processed=False),\n",
       " JobUrl(company='zendesk', job_name='senior-data-scientist', url='/jobs/zendesk/senior-data-scientist', processed=False),\n",
       " JobUrl(company='bond', job_name='data-scientist-8fd126', url='/jobs/bond/data-scientist-8fd126', processed=False),\n",
       " JobUrl(company='nvidia', job_name='principal-firmware-engineer-data-center-server-management', url='/jobs/nvidia/principal-firmware-engineer-data-center-server-management', processed=False),\n",
       " JobUrl(company='crowdstrike', job_name='principal-software-engineer-data-platform-remote', url='/jobs/crowdstrike/principal-software-engineer-data-platform-remote', processed=False),\n",
       " JobUrl(company='kyndryl', job_name='data-analysis', url='/jobs/kyndryl/data-analysis', processed=False),\n",
       " JobUrl(company='sothebys', job_name='senior-data-scientist', url='/jobs/sothebys/senior-data-scientist', processed=False),\n",
       " JobUrl(company='hatch', job_name='senior-data-scientist', url='/jobs/hatch/senior-data-scientist', processed=False),\n",
       " JobUrl(company='coinbase', job_name='senior-data-scientist-fe1ba7', url='/jobs/coinbase/senior-data-scientist-fe1ba7', processed=False),\n",
       " JobUrl(company='arcadia', job_name='director-data-integration', url='/jobs/arcadia/director-data-integration', processed=False),\n",
       " JobUrl(company='geaerospace', job_name='staff-data-scientist', url='/jobs/geaerospace/staff-data-scientist', processed=False),\n",
       " JobUrl(company='wealthfront', job_name='data-scientist-growth-7cd26f', url='/jobs/wealthfront/data-scientist-growth-7cd26f', processed=False),\n",
       " JobUrl(company='kyndryl', job_name='lead-data-analysis', url='/jobs/kyndryl/lead-data-analysis', processed=False),\n",
       " JobUrl(company='kyndryl', job_name='clouddata-architect', url='/jobs/kyndryl/clouddata-architect', processed=False),\n",
       " JobUrl(company='arcadia', job_name='devops-engineer-finops', url='/jobs/arcadia/devops-engineer-finops', processed=False),\n",
       " JobUrl(company='atlassian', job_name='senior-manager-data-science-6df833', url='/jobs/atlassian/senior-manager-data-science-6df833', processed=False),\n",
       " JobUrl(company='nvidia', job_name='image-and-data-processing-libraries-intern', url='/jobs/nvidia/image-and-data-processing-libraries-intern', processed=False),\n",
       " JobUrl(company='kyndryl', job_name='senior-data-analyst', url='/jobs/kyndryl/senior-data-analyst', processed=False),\n",
       " JobUrl(company='crowdstrike', job_name='data-analyst-remote-rou-faebd2', url='/jobs/crowdstrike/data-analyst-remote-rou-faebd2', processed=False),\n",
       " JobUrl(company='himshers', job_name='lead-data-scientist', url='/jobs/himshers/lead-data-scientist', processed=False),\n",
       " JobUrl(company='worldwidetechnology', job_name='data-scientistcosta-rica', url='/jobs/worldwidetechnology/data-scientistcosta-rica', processed=False),\n",
       " JobUrl(company='arcadia', job_name='senior-engineer-data-platforms', url='/jobs/arcadia/senior-engineer-data-platforms', processed=False),\n",
       " JobUrl(company='nvidia', job_name='solutions-architect-data-center-infrastructure-367810', url='/jobs/nvidia/solutions-architect-data-center-infrastructure-367810', processed=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# appel de la methode une 1ere fois\n",
    "from pprint import pprint\n",
    "\n",
    "joburls_ls: List[JobUrl] = []\n",
    "count: int = 0\n",
    "next_page: str | None= None\n",
    "query: str = \"Data engineer\"\n",
    "has_more: bool = True\n",
    "\n",
    "while has_more :\n",
    "# while count < 10:\n",
    "\n",
    "    job_api_reponse: List[Dict] = await request_jobs_api(query=query, next_page=next_page)\n",
    "    scrap_result: Dict = await scrap_jobs(job_api_reponse)\n",
    "\n",
    "    has_more: str = scrap_result.get('has_more')\n",
    "    next_page: str = scrap_result.get('next_page')\n",
    "    joburls_ls.extend(scrap_result.get('job_urls'))\n",
    "\n",
    "    print(has_more)\n",
    "    print(next_page)\n",
    "\n",
    "    # simulation has_more=False apres 5 iterations\n",
    "    count+=1\n",
    "    if count > 2:\n",
    "        has_more = False\n",
    "\n",
    "print(len(joburls_ls))\n",
    "joburls_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction des informaton de chaque job\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
